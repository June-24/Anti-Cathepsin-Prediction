{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, SeparableConv1D, MaxPooling1D, Dropout, Flatten, Dense, BatchNormalization, Add, Activation, GlobalAveragePooling1D\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input-output file names\n",
    "datasets = {\n",
    "    \"B\": {\"input\": \"./Training_data/input_cathepsin_B.csv\", \"output\": \"./Training_data/output_cathepsin_B.csv\"},\n",
    "    \"S\": {\"input\": \"./Training_data/input_cathepsin_S.csv\", \"output\": \"./Training_data/output_cathepsin_S.csv\"},\n",
    "    \"D\": {\"input\": \"./Training_data/input_cathepsin_D.csv\", \"output\": \"./Training_data/output_cathepsin_D.csv\"},\n",
    "    \"K\": {\"input\": \"./Training_data/input_cathepsin_K.csv\", \"output\": \"./Training_data/output_cathepsin_K.csv\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data\n",
    "def preprocess_data(input_path, output_path):\n",
    "    # Load input and output data\n",
    "    input_data = pd.read_csv(input_path, header=None, skiprows=1)\n",
    "    output_data = pd.read_csv(output_path, header=None, skiprows=1)\n",
    "\n",
    "    # Normalize input data\n",
    "    scaler = MinMaxScaler()\n",
    "    input_data_normalized = scaler.fit_transform(input_data)\n",
    "\n",
    "    # Handle NaN values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    input_data_imputed = imputer.fit_transform(input_data_normalized)\n",
    "\n",
    "    # One-hot encode output data\n",
    "    output_labels = output_data[0].values\n",
    "    output_data_encoded = pd.get_dummies(output_labels, dtype=np.float32).values\n",
    "\n",
    "    # Reshape input data for CNN\n",
    "    input_data_reshaped = input_data_imputed.reshape((input_data_imputed.shape[0], input_data_imputed.shape[1], 1))\n",
    "\n",
    "    return input_data_reshaped, output_data_encoded\n",
    "\n",
    "# Function to build the model\n",
    "\n",
    "\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset B...\n",
      "Training model for dataset B...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Moham\\OneDrive\\Desktop\\Anti-Cathepsin Prediction\\ALLCAT\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6057 - loss: 0.8969 - val_accuracy: 0.8104 - val_loss: 0.5123\n",
      "Epoch 2/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8021 - loss: 0.4863 - val_accuracy: 0.8428 - val_loss: 0.4090\n",
      "Epoch 3/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8403 - loss: 0.3964 - val_accuracy: 0.8572 - val_loss: 0.3516\n",
      "Epoch 4/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8655 - loss: 0.3418 - val_accuracy: 0.8760 - val_loss: 0.3590\n",
      "Epoch 5/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8949 - loss: 0.2975 - val_accuracy: 0.8645 - val_loss: 0.3277\n",
      "Epoch 6/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9026 - loss: 0.2558 - val_accuracy: 0.8904 - val_loss: 0.2955\n",
      "Epoch 7/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9090 - loss: 0.2520 - val_accuracy: 0.9027 - val_loss: 0.2637\n",
      "Epoch 8/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9177 - loss: 0.2211 - val_accuracy: 0.9214 - val_loss: 0.2406\n",
      "Epoch 9/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9267 - loss: 0.1919 - val_accuracy: 0.9236 - val_loss: 0.2120\n",
      "Epoch 10/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9368 - loss: 0.1739 - val_accuracy: 0.9185 - val_loss: 0.2381\n",
      "Epoch 11/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9377 - loss: 0.1768 - val_accuracy: 0.9344 - val_loss: 0.1885\n",
      "Epoch 12/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9515 - loss: 0.1405 - val_accuracy: 0.9308 - val_loss: 0.2027\n",
      "Epoch 13/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9478 - loss: 0.1471 - val_accuracy: 0.9142 - val_loss: 0.2450\n",
      "Epoch 14/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9494 - loss: 0.1331 - val_accuracy: 0.9358 - val_loss: 0.1980\n",
      "Epoch 15/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9507 - loss: 0.1388 - val_accuracy: 0.9430 - val_loss: 0.2055\n",
      "Epoch 16/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9521 - loss: 0.1307 - val_accuracy: 0.9438 - val_loss: 0.1889\n",
      "Epoch 17/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9559 - loss: 0.1176 - val_accuracy: 0.9279 - val_loss: 0.2004\n",
      "Epoch 18/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9525 - loss: 0.1149 - val_accuracy: 0.9423 - val_loss: 0.1627\n",
      "Epoch 19/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9592 - loss: 0.1236 - val_accuracy: 0.9394 - val_loss: 0.1970\n",
      "Epoch 20/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9624 - loss: 0.1116 - val_accuracy: 0.9438 - val_loss: 0.1920\n",
      "44/44 - 0s - 2ms/step - accuracy: 0.9438 - loss: 0.1920\n",
      "Test accuracy for dataset B: 0.9437634944915771\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy for dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lime_tabular\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Define the LIME explainer\u001b[39;00m\n\u001b[0;32m     44\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(\n\u001b[0;32m     45\u001b[0m     X_train\u001b[38;5;241m.\u001b[39mreshape(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# Flatten for LIME\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     discretize_continuous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     50\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lime'"
     ]
    }
   ],
   "source": [
    "# Process each dataset\n",
    "for key, paths in datasets.items():\n",
    "    print(f\"Processing dataset {key}...\")\n",
    "\n",
    "    # Preprocess the data\n",
    "    input_data, output_data = preprocess_data(paths['input'], paths['output'])\n",
    "\n",
    "    # Apply SMOTE to balance classes\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(\n",
    "        input_data.reshape(input_data.shape[0], -1), output_data\n",
    "    )\n",
    "    X_resampled = X_resampled.reshape((X_resampled.shape[0], input_data.shape[1], 1))\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "    # Build the model\n",
    "    model = build_model(X_train.shape[1:], num_classes=output_data.shape[1])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"Training model for dataset {key}...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=20, \n",
    "        batch_size=16, \n",
    "        validation_data=(X_test, y_test), \n",
    "        # callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f\"Test accuracy for dataset {key}: {test_acc}\")\n",
    "    from lime import lime_tabular\n",
    "\n",
    "    # Define the LIME explainer\n",
    "    explainer = lime_tabular.LimeTabularExplainer(\n",
    "        X_train.reshape(X_train.shape[0], -1),  # Flatten for LIME\n",
    "        mode=\"classification\",\n",
    "        training_labels=np.argmax(y_train, axis=1),  # Convert one-hot to class labels\n",
    "        feature_names=[f\"Feature_{i}\" for i in range(X_train.shape[1])],\n",
    "        discretize_continuous=True\n",
    "    )\n",
    "\n",
    "    # Select a test sample to explain\n",
    "    sample_index = 0  # Change this to any test sample index you want\n",
    "    sample = X_test[sample_index].reshape(1, -1)  # Flatten for LIME\n",
    "\n",
    "    # Generate LIME explanation\n",
    "    exp = explainer.explain_instance(\n",
    "        sample[0],  # Flattened sample\n",
    "        model.predict,  # Your trained model's prediction function\n",
    "        num_features=10  # Number of top features to show\n",
    "    )\n",
    "\n",
    "    # Visualize the explanation\n",
    "    exp.show_in_notebook()  # Shows in Jupyter Notebook (or)\n",
    "    exp.as_pyplot_figure()\n",
    "\n",
    "\n",
    "    # Predict and generate classification report\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    print(f\"Classification Report for dataset {key}:\")\n",
    "    print(classification_report(np.argmax(y_test, axis=1), y_pred_classes))\n",
    "    # Save the trained model\n",
    "    model.save(f'./Saved_Models/model_{key}.h5')\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f\"Accuracy for Dataset {key}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALLCAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
