{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for B with threshold 0.01 reduced to 186 features. Saved to './reduced_features/Variance/B/reduced_desc_Variance_B_1.csv'.\n",
      "Dataset for B with threshold 0.1 reduced to 141 features. Saved to './reduced_features/Variance/B/reduced_desc_Variance_B_2.csv'.\n",
      "Dataset for B with threshold 0.5 reduced to 114 features. Saved to './reduced_features/Variance/B/reduced_desc_Variance_B_3.csv'.\n",
      "Dataset for B with threshold 0.6 reduced to 113 features. Saved to './reduced_features/Variance/B/reduced_desc_Variance_B_4.csv'.\n",
      "Dataset for B with threshold 0.7 reduced to 112 features. Saved to './reduced_features/Variance/B/reduced_desc_Variance_B_5.csv'.\n",
      "Dataset for B with threshold 0.8 reduced to 108 features. Saved to './reduced_features/Variance/B/reduced_desc_Variance_B_6.csv'.\n",
      "Dataset for S with threshold 0.01 reduced to 169 features. Saved to './reduced_features/Variance/S/reduced_desc_Variance_S_1.csv'.\n",
      "Dataset for S with threshold 0.1 reduced to 139 features. Saved to './reduced_features/Variance/S/reduced_desc_Variance_S_2.csv'.\n",
      "Dataset for S with threshold 0.5 reduced to 119 features. Saved to './reduced_features/Variance/S/reduced_desc_Variance_S_3.csv'.\n",
      "Dataset for S with threshold 0.6 reduced to 113 features. Saved to './reduced_features/Variance/S/reduced_desc_Variance_S_4.csv'.\n",
      "Dataset for S with threshold 0.7 reduced to 110 features. Saved to './reduced_features/Variance/S/reduced_desc_Variance_S_5.csv'.\n",
      "Dataset for S with threshold 0.8 reduced to 109 features. Saved to './reduced_features/Variance/S/reduced_desc_Variance_S_6.csv'.\n",
      "Dataset for D with threshold 0.01 reduced to 176 features. Saved to './reduced_features/Variance/D/reduced_desc_Variance_D_1.csv'.\n",
      "Dataset for D with threshold 0.1 reduced to 138 features. Saved to './reduced_features/Variance/D/reduced_desc_Variance_D_2.csv'.\n",
      "Dataset for D with threshold 0.5 reduced to 117 features. Saved to './reduced_features/Variance/D/reduced_desc_Variance_D_3.csv'.\n",
      "Dataset for D with threshold 0.6 reduced to 114 features. Saved to './reduced_features/Variance/D/reduced_desc_Variance_D_4.csv'.\n",
      "Dataset for D with threshold 0.7 reduced to 113 features. Saved to './reduced_features/Variance/D/reduced_desc_Variance_D_5.csv'.\n",
      "Dataset for D with threshold 0.8 reduced to 113 features. Saved to './reduced_features/Variance/D/reduced_desc_Variance_D_6.csv'.\n",
      "Dataset for K with threshold 0.01 reduced to 175 features. Saved to './reduced_features/Variance/K/reduced_desc_Variance_K_1.csv'.\n",
      "Dataset for K with threshold 0.1 reduced to 138 features. Saved to './reduced_features/Variance/K/reduced_desc_Variance_K_2.csv'.\n",
      "Dataset for K with threshold 0.5 reduced to 114 features. Saved to './reduced_features/Variance/K/reduced_desc_Variance_K_3.csv'.\n",
      "Dataset for K with threshold 0.6 reduced to 112 features. Saved to './reduced_features/Variance/K/reduced_desc_Variance_K_4.csv'.\n",
      "Dataset for K with threshold 0.7 reduced to 112 features. Saved to './reduced_features/Variance/K/reduced_desc_Variance_K_5.csv'.\n",
      "Dataset for K with threshold 0.8 reduced to 109 features. Saved to './reduced_features/Variance/K/reduced_desc_Variance_K_6.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import os\n",
    "\n",
    "# Define parameters\n",
    "types = [\"B\", \"S\", \"D\", \"K\"]\n",
    "thresholds = [0.01, 0.1, 0.5, 0.6, 0.7, 0.8]\n",
    "threshold_suffix = {0.01: \"1\", 0.1: \"2\", 0.5: \"3\", 0.6: \"4\", 0.7: \"5\", 0.8: \"6\"}  # Mapping thresholds to suffixes\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"./reduced_features/Variance\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each type and threshold\n",
    "for t in types:\n",
    "    input_file_path = f\"./features/input_cathepsin_{t}.csv\"  # Input file for each type\n",
    "    data = pd.read_csv(input_file_path)  # Load with header to retain feature names\n",
    "\n",
    "    # Replace NaN values with column averages\n",
    "    data = data.fillna(data.mean())\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Apply Variance Threshold\n",
    "        selector = VarianceThreshold(threshold=threshold)\n",
    "        data_reduced = selector.fit_transform(data)\n",
    "\n",
    "        # Get selected feature names\n",
    "        selected_feature_names = data.columns[selector.get_support()]\n",
    "\n",
    "        # Convert reduced data to DataFrame with feature names\n",
    "        data_reduced_df = pd.DataFrame(data_reduced, columns=selected_feature_names)\n",
    "\n",
    "        # Define output file path\n",
    "        output_file_path = f\"{output_dir}/{t}/reduced_desc_Variance_{t}_{threshold_suffix[threshold]}.csv\"\n",
    "        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "        # Save the reduced dataset\n",
    "        data_reduced_df.to_csv(output_file_path, index=False)\n",
    "        print(f\"Dataset for {t} with threshold {threshold} reduced to {data_reduced.shape[1]} features. Saved to '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation-Based Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for B with threshold 0.9 reduced to 168 features. Saved to './reduced_features/Correlation/B/reduced_desc_Correlation_B_1.csv'.\n",
      "Dataset for B with threshold 0.8 reduced to 144 features. Saved to './reduced_features/Correlation/B/reduced_desc_Correlation_B_2.csv'.\n",
      "Dataset for B with threshold 0.7 reduced to 127 features. Saved to './reduced_features/Correlation/B/reduced_desc_Correlation_B_3.csv'.\n",
      "Dataset for B with threshold 0.6 reduced to 105 features. Saved to './reduced_features/Correlation/B/reduced_desc_Correlation_B_4.csv'.\n",
      "Dataset for B with threshold 0.5 reduced to 81 features. Saved to './reduced_features/Correlation/B/reduced_desc_Correlation_B_5.csv'.\n",
      "Dataset for B with threshold 0.4 reduced to 56 features. Saved to './reduced_features/Correlation/B/reduced_desc_Correlation_B_6.csv'.\n",
      "Dataset for S with threshold 0.9 reduced to 172 features. Saved to './reduced_features/Correlation/S/reduced_desc_Correlation_S_1.csv'.\n",
      "Dataset for S with threshold 0.8 reduced to 142 features. Saved to './reduced_features/Correlation/S/reduced_desc_Correlation_S_2.csv'.\n",
      "Dataset for S with threshold 0.7 reduced to 127 features. Saved to './reduced_features/Correlation/S/reduced_desc_Correlation_S_3.csv'.\n",
      "Dataset for S with threshold 0.6 reduced to 93 features. Saved to './reduced_features/Correlation/S/reduced_desc_Correlation_S_4.csv'.\n",
      "Dataset for S with threshold 0.5 reduced to 80 features. Saved to './reduced_features/Correlation/S/reduced_desc_Correlation_S_5.csv'.\n",
      "Dataset for S with threshold 0.4 reduced to 62 features. Saved to './reduced_features/Correlation/S/reduced_desc_Correlation_S_6.csv'.\n",
      "Dataset for D with threshold 0.9 reduced to 167 features. Saved to './reduced_features/Correlation/D/reduced_desc_Correlation_D_1.csv'.\n",
      "Dataset for D with threshold 0.8 reduced to 139 features. Saved to './reduced_features/Correlation/D/reduced_desc_Correlation_D_2.csv'.\n",
      "Dataset for D with threshold 0.7 reduced to 119 features. Saved to './reduced_features/Correlation/D/reduced_desc_Correlation_D_3.csv'.\n",
      "Dataset for D with threshold 0.6 reduced to 98 features. Saved to './reduced_features/Correlation/D/reduced_desc_Correlation_D_4.csv'.\n",
      "Dataset for D with threshold 0.5 reduced to 80 features. Saved to './reduced_features/Correlation/D/reduced_desc_Correlation_D_5.csv'.\n",
      "Dataset for D with threshold 0.4 reduced to 63 features. Saved to './reduced_features/Correlation/D/reduced_desc_Correlation_D_6.csv'.\n",
      "Dataset for K with threshold 0.9 reduced to 171 features. Saved to './reduced_features/Correlation/K/reduced_desc_Correlation_K_1.csv'.\n",
      "Dataset for K with threshold 0.8 reduced to 149 features. Saved to './reduced_features/Correlation/K/reduced_desc_Correlation_K_2.csv'.\n",
      "Dataset for K with threshold 0.7 reduced to 129 features. Saved to './reduced_features/Correlation/K/reduced_desc_Correlation_K_3.csv'.\n",
      "Dataset for K with threshold 0.6 reduced to 95 features. Saved to './reduced_features/Correlation/K/reduced_desc_Correlation_K_4.csv'.\n",
      "Dataset for K with threshold 0.5 reduced to 76 features. Saved to './reduced_features/Correlation/K/reduced_desc_Correlation_K_5.csv'.\n",
      "Dataset for K with threshold 0.4 reduced to 62 features. Saved to './reduced_features/Correlation/K/reduced_desc_Correlation_K_6.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "types = [\"B\", \"S\", \"D\", \"K\"]\n",
    "thresholds = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4]\n",
    "threshold_suffix = {0.9: \"1\", 0.8: \"2\", 0.7: \"3\", 0.6: \"4\", 0.5: \"5\", 0.4: \"6\"}  # Mapping thresholds to suffixes\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"./reduced_features/Correlation\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for t in types:\n",
    "    \n",
    "    # Step 1: Load the CSV file with the header\n",
    "    file_path = f\"./features/input_cathepsin_{t}.csv\"  # Replace with your file path\n",
    "    data = pd.read_csv(file_path)  # Load with header to retain feature names\n",
    "\n",
    "    # Step 2: Ensure only numeric columns are processed\n",
    "    data_numeric = data.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric to NaN\n",
    "\n",
    "    # Step 3: Replace NaN values with column averages\n",
    "    data_numeric = data_numeric.fillna(data_numeric.mean())\n",
    "\n",
    "    # Step 4: Calculate the correlation matrix\n",
    "    correlation_matrix = data_numeric.corr().abs()\n",
    "\n",
    "    # Step 5: Identify and remove highly correlated features\n",
    "    # Use the upper triangle of the correlation matrix\n",
    "    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    for threshold in thresholds:\n",
    "\n",
    "        # Find columns to drop based on a correlation threshold (e.g., 0.9)\n",
    "        correlation_threshold = threshold # Adjust threshold as needed\n",
    "        to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
    "\n",
    "        # Drop the highly correlated columns\n",
    "        data_reduced = data.drop(columns=to_drop)  # Use original 'data' to retain feature names\n",
    "\n",
    "\n",
    "        # Step 6: Save the reduced dataset with feature names\n",
    "        output_file_path = f\"./reduced_features/Correlation/{t}/reduced_desc_Correlation_{t}_{threshold_suffix[threshold]}.csv\"\n",
    "        data_reduced.to_csv(output_file_path, index=False)\n",
    "        print(f\"Dataset for {t} with threshold {threshold} reduced to {data_reduced.shape[1]} features. Saved to '{output_file_path}'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE (desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for B with 150 selected features saved to './reduced_features/RFE/B/reduced_desc_RFE_B_1.csv'.\n",
      "Dataset for B with 130 selected features saved to './reduced_features/RFE/B/reduced_desc_RFE_B_2.csv'.\n",
      "Dataset for B with 90 selected features saved to './reduced_features/RFE/B/reduced_desc_RFE_B_3.csv'.\n",
      "Dataset for B with 50 selected features saved to './reduced_features/RFE/B/reduced_desc_RFE_B_4.csv'.\n",
      "Dataset for B with 30 selected features saved to './reduced_features/RFE/B/reduced_desc_RFE_B_5.csv'.\n",
      "Dataset for B with 20 selected features saved to './reduced_features/RFE/B/reduced_desc_RFE_B_6.csv'.\n",
      "Dataset for S with 150 selected features saved to './reduced_features/RFE/S/reduced_desc_RFE_S_1.csv'.\n",
      "Dataset for S with 130 selected features saved to './reduced_features/RFE/S/reduced_desc_RFE_S_2.csv'.\n",
      "Dataset for S with 90 selected features saved to './reduced_features/RFE/S/reduced_desc_RFE_S_3.csv'.\n",
      "Dataset for S with 50 selected features saved to './reduced_features/RFE/S/reduced_desc_RFE_S_4.csv'.\n",
      "Dataset for S with 30 selected features saved to './reduced_features/RFE/S/reduced_desc_RFE_S_5.csv'.\n",
      "Dataset for S with 20 selected features saved to './reduced_features/RFE/S/reduced_desc_RFE_S_6.csv'.\n",
      "Dataset for D with 150 selected features saved to './reduced_features/RFE/D/reduced_desc_RFE_D_1.csv'.\n",
      "Dataset for D with 130 selected features saved to './reduced_features/RFE/D/reduced_desc_RFE_D_2.csv'.\n",
      "Dataset for D with 90 selected features saved to './reduced_features/RFE/D/reduced_desc_RFE_D_3.csv'.\n",
      "Dataset for D with 50 selected features saved to './reduced_features/RFE/D/reduced_desc_RFE_D_4.csv'.\n",
      "Dataset for D with 30 selected features saved to './reduced_features/RFE/D/reduced_desc_RFE_D_5.csv'.\n",
      "Dataset for D with 20 selected features saved to './reduced_features/RFE/D/reduced_desc_RFE_D_6.csv'.\n",
      "Dataset for K with 150 selected features saved to './reduced_features/RFE/K/reduced_desc_RFE_K_1.csv'.\n",
      "Dataset for K with 130 selected features saved to './reduced_features/RFE/K/reduced_desc_RFE_K_2.csv'.\n",
      "Dataset for K with 90 selected features saved to './reduced_features/RFE/K/reduced_desc_RFE_K_3.csv'.\n",
      "Dataset for K with 50 selected features saved to './reduced_features/RFE/K/reduced_desc_RFE_K_4.csv'.\n",
      "Dataset for K with 30 selected features saved to './reduced_features/RFE/K/reduced_desc_RFE_K_5.csv'.\n",
      "Dataset for K with 20 selected features saved to './reduced_features/RFE/K/reduced_desc_RFE_K_6.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "import os\n",
    "\n",
    "# Define parameters\n",
    "types = [\"B\", \"S\", \"D\", \"K\"]\n",
    "num_features_list = [150, 130, 90, 50, 30, 20]\n",
    "feature_suffix = {150: \"1\", 130: \"2\", 90: \"3\", 50: \"4\", 30: \"5\", 20: \"6\"}  # Mapping feature count to suffix\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"./reduced_features/RFE\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each type and feature count\n",
    "for t in types:\n",
    "    input_file_path = f\"./features/input_cathepsin_{t}.csv\"  # Input file for each type\n",
    "    data = pd.read_csv(input_file_path)  # Load with header to retain feature names\n",
    "\n",
    "    # Ensure numeric data and handle missing values\n",
    "    data_numeric = data.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric to NaN\n",
    "    data_numeric = data_numeric.fillna(data_numeric.mean())  # Replace NaN with column averages\n",
    "\n",
    "    # Create a placeholder target (mean of all features)\n",
    "    placeholder_target = data_numeric.mean(axis=1)\n",
    "\n",
    "    # Initialize a regression model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    for num_features in num_features_list:\n",
    "        # Apply Recursive Feature Elimination (RFE)\n",
    "        rfe = RFE(model, n_features_to_select=num_features)\n",
    "        rfe.fit(data_numeric, placeholder_target)\n",
    "\n",
    "        # Get selected features\n",
    "        selected_features = rfe.support_  # Boolean mask of selected features\n",
    "        selected_feature_names = data.columns[selected_features]  # Retain selected feature names\n",
    "        data_reduced = data.loc[:, selected_feature_names]\n",
    "\n",
    "        # Define output file path\n",
    "        output_file_path = f\"{output_dir}/{t}/reduced_desc_RFE_{t}_{feature_suffix[num_features]}.csv\"\n",
    "        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "        # Save the reduced dataset\n",
    "        data_reduced.to_csv(output_file_path, index=False)\n",
    "        print(f\"Dataset for {t} with {num_features} selected features saved to '{output_file_path}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALLCAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
